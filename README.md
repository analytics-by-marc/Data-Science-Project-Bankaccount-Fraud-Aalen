# Data-Science-Project-Bankaccount-Fraud-Aalen
Erstellt mit Python Version 3.10.18

ðŸ” Projektziele

    Mindestens 75 % der BetrugsfÃ¤lle sollen erkannt werden
    
    Aufbau von vollstÃ¤ndigen Fraudâ€‘Detectionâ€‘Pipelines
    
    Umgang mit stark unausgeglichenen Daten (â‰ˆ 1.1â€¯% Fraud)

    Vergleich verschiedener Machine Learning Modelle

    Optimierung von Precision/Recall (Score-Parameter) durch die Optimierung des Thresholds

    Visualisierung der Modellperformance

ðŸ“‚ Inhalt

    Datenvorbereitung

        Laden & PrÃ¼fen des Datensatzes

        Umgang mit fehlenden Werten

        Entfernen von Spalten mit >50â€¯% Missing Values

        Encoding & Skalierung

        Varianzanalyse und Entfernen von Spalten

        Entfernen von Features mit hoher Korrelation

    Datenanalyse

        Verteilungen aller Features

        Crosstabs fÃ¼r geschÃ¼tzte Attribute

        Analyse der Rate an BetrugsfÃ¤llen

        Densityâ€‘Plots fÃ¼r numerische Features

    Modelle

        Logistic Regression (mit GridSearchCV + SMOTEâ€‘Varianten)

        Decision Trees

        Random Forest

        Gradient Boosting

        XGBoost

        LightGBM

        Neuronales Netz (TensorFlow/Keras)

    Modellvergleich

        ROCâ€‘AUC

        Precision

        Recall

        F1â€‘Score

        Precisionâ€‘Recallâ€‘Kurven

        Thresholdâ€‘Optimierung

ðŸ§¹ Datenvorbereitung
1. Entfernen unnÃ¶tiger Spalten
python

X = df_bankaccounts.drop(columns=["x1", "x2", "fraud_bool"])
y = df_bankaccounts["fraud_bool"]

2. Missing Values erkennen & behandeln

    Negative Werte â†’ als fehlend markiert

    Spalten mit >50â€¯% Missing Values entfernt

    Medianâ€‘Imputation fÃ¼r verbleibende Spalten

3. Encoding

    Oneâ€‘Hotâ€‘Encoding fÃ¼r kategoriale Variablen

    Manuelles Oneâ€‘Hotâ€‘Encoding fÃ¼r numerische Kategorien (income, age, month)

4. Skalierung

MinMaxScaler auf alle numerischen Features.
5. Feature Selection

    Entfernen stark korrelierter Features

    VarianceThreshold (0.01)

ðŸ“Š Explorative Datenanalyse

    Fraudâ€‘Rate: 1.103â€¯%

    Barplots fÃ¼r Einkommen, Alter, Employment Status

    Crosstabs: Fraudâ€‘Rate pro Kategorie

    Histogramme & KDEâ€‘Plots fÃ¼r numerische Features

ðŸ¤– Modelle & Ergebnisse
Logistic Regression

    GridSearchCV mit:

        L1/L2â€‘Regularisierung

        SMOTE / BorderlineSMOTE / None

        Verschiedene Solver

    Bestes Ergebnis:

        ROCâ€‘AUC â‰ˆ 0.856

        Recall stark abhÃ¤ngig vom Threshold

LightGBM (bestes Modell)

    Hyperparameterâ€‘Tuning Ã¼ber GridSearchCV

    Bestes Ergebnis:

        ROCâ€‘AUC â‰ˆ 0.875

        Sehr gute Trennung trotz Imbalance

        Recall bis 75â€¯% erreichbar durch Thresholdâ€‘Tuning

Neuronales Netz

    2 Hidden Layers (64 â†’ 32 Neuronen)

    BatchNorm + Dropout

    Class Weights fÃ¼r Imbalance

    Ergebnis:

        ROCâ€‘AUC â‰ˆ 0.862

        Sehr stabil, aber LightGBM leicht besser

ðŸŽ¯ Thresholdâ€‘Optimierung

FÃ¼r alle Modelle wurden Precisionâ€‘Recallâ€‘Kurven analysiert:

    Schnittpunkt Precision = Recall â†’ optimaler F1â€‘Threshold

    ZusÃ¤tzlich: Threshold fÃ¼r Recall = 0.75 berechnet

    Visualisierung aller Kurven

ðŸ“ˆ Modellvergleich (Testdaten)
Modell	ROCâ€‘AUC	Precision	Recall	F1
Logistic Regression	~0.86	sehr niedrig	hoch (mit Threshold)	niedrig
LightGBM	~0.88	moderat	hoch	bester F1
Neural Network	~0.86	niedrig	hoch	Ã¤hnlich LogReg

LightGBM ist das beste Modell im Projekt.
ðŸ–¼ Visualisierungen

Das Projekt enthÃ¤lt u.â€¯a.:

    ROCâ€‘Kurven

    Precisionâ€‘Recallâ€‘Kurven

    Confusion Matrices

    Featureâ€‘Verteilungen

    F1â€‘Scoreâ€‘Thresholdâ€‘Kurven

    Modellvergleich als Balkendiagramm

ðŸ§  Technologien

    Python

    Pandas, NumPy

    Scikitâ€‘Learn

    XGBoost, LightGBM

    TensorFlow / Keras

    Matplotlib, Seaborn

    Imbalancedâ€‘Learn

ðŸš€ Fazit

Dieses Projekt zeigt einen vollstÃ¤ndigen Endâ€‘toâ€‘Endâ€‘Machineâ€‘Learningâ€‘Workflow fÃ¼r Fraud Detection mit stark unausgeglichenen Daten.
Durch systematische Datenvorbereitung, Modellvergleich und Thresholdâ€‘Optimierung konnte ein leistungsstarkes Modell (LightGBM) entwickelt werden.

